import { ImageAnnotatorClient } from '@google-cloud/vision'

export interface ContentDetectionResult {
  isInappropriate: boolean
  confidence: number
  categories: {
    adult: 'VERY_UNLIKELY' | 'UNLIKELY' | 'POSSIBLE' | 'LIKELY' | 'VERY_LIKELY'
    violence: 'VERY_UNLIKELY' | 'UNLIKELY' | 'POSSIBLE' | 'LIKELY' | 'VERY_LIKELY'
    racy: 'VERY_UNLIKELY' | 'UNLIKELY' | 'POSSIBLE' | 'LIKELY' | 'VERY_LIKELY'
  }
  details: string
}

export class ContentDetector {
  private client: ImageAnnotatorClient
  private readonly INAPPROPRIATE_THRESHOLD = 'POSSIBLE' // Considera inapropriado a partir de "POSSIBLE"

  constructor() {
    // Inicializa o cliente Google Vision API
    this.client = new ImageAnnotatorClient({
      // As credenciais ser√£o carregadas automaticamente via GOOGLE_APPLICATION_CREDENTIALS
      // ou via service account key
    })
  }

  /**
   * Analisa uma imagem para detectar conte√∫do inapropriado
   * @param imageBuffer Buffer da imagem
   * @returns Resultado da an√°lise
   */
  async analyzeImage(imageBuffer: Buffer): Promise<ContentDetectionResult> {
    try {
      console.log('üîç CONTENT DETECTOR: Iniciando an√°lise de imagem...')
      
      // Fazer a requisi√ß√£o para o Google Vision API
      const [result] = await this.client.safeSearchDetection({
        image: { content: imageBuffer }
      })

      const safeSearch = result.safeSearchAnnotation
      
      if (!safeSearch) {
        console.log('‚ö†Ô∏è CONTENT DETECTOR: Nenhum resultado de safe search retornado')
        return {
          isInappropriate: false,
          confidence: 0,
          categories: {
            adult: 'VERY_UNLIKELY',
            violence: 'VERY_UNLIKELY',
            racy: 'VERY_UNLIKELY'
          },
          details: 'An√°lise n√£o dispon√≠vel'
        }
      }

      console.log('üìä CONTENT DETECTOR: Resultado da an√°lise:', {
        adult: safeSearch.adult,
        violence: safeSearch.violence,
        racy: safeSearch.racy
      })

      // Verificar se algum conte√∫do √© inapropriado
      const isInappropriate = this.isContentInappropriate(safeSearch)
      
      // Calcular confian√ßa baseada na severidade
      const confidence = this.calculateConfidence(safeSearch)
      
      // Gerar detalhes da an√°lise
      const details = this.generateDetails(safeSearch)

      console.log(`‚úÖ CONTENT DETECTOR: An√°lise conclu√≠da - Inapropriado: ${isInappropriate}, Confian√ßa: ${confidence}%`)

      return {
        isInappropriate,
        confidence,
        categories: {
          adult: safeSearch.adult || 'VERY_UNLIKELY',
          violence: safeSearch.violence || 'VERY_UNLIKELY',
          racy: safeSearch.racy || 'VERY_UNLIKELY'
        },
        details
      }

    } catch (error) {
      console.error('‚ùå CONTENT DETECTOR: Erro na an√°lise:', error)
      
      // Em caso de erro, retornar resultado seguro (n√£o inapropriado)
      return {
        isInappropriate: false,
        confidence: 0,
        categories: {
          adult: 'VERY_UNLIKELY',
          violence: 'VERY_UNLIKELY',
          racy: 'VERY_UNLIKELY'
        },
        details: `Erro na an√°lise: ${error instanceof Error ? error.message : 'Erro desconhecido'}`
      }
    }
  }

  /**
   * Verifica se o conte√∫do √© inapropriado baseado nos resultados
   */
  private isContentInappropriate(safeSearch: any): boolean {
    const { adult, violence, racy } = safeSearch
    
    // Considera inapropriado se qualquer categoria for POSSIBLE ou superior
    return this.isLikelyInappropriate(adult) || 
           this.isLikelyInappropriate(violence) || 
           this.isLikelyInappropriate(racy)
  }

  /**
   * Verifica se uma categoria espec√≠fica √© provavelmente inapropriada
   */
  private isLikelyInappropriate(level: string): boolean {
    const levels = ['VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY']
    const thresholdIndex = levels.indexOf(this.INAPPROPRIATE_THRESHOLD)
    const currentIndex = levels.indexOf(level)
    
    return currentIndex >= thresholdIndex
  }

  /**
   * Calcula a confian√ßa baseada na severidade do conte√∫do
   */
  private calculateConfidence(safeSearch: any): number {
    const { adult, violence, racy } = safeSearch
    const levels = ['VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY']
    
    let maxConfidence = 0
    
    // Calcular confian√ßa para cada categoria
    const adultConfidence = this.getConfidenceValue(adult, levels)
    const violenceConfidence = this.getConfidenceValue(violence, levels)
    const racyConfidence = this.getConfidenceValue(racy, levels)
    
    maxConfidence = Math.max(adultConfidence, violenceConfidence, racyConfidence)
    
    return Math.round(maxConfidence)
  }

  /**
   * Converte o n√≠vel de detec√ß√£o em valor de confian√ßa (0-100)
   */
  private getConfidenceValue(level: string, levels: string[]): number {
    const index = levels.indexOf(level)
    if (index === -1) return 0
    
    // VERY_UNLIKELY = 0%, UNLIKELY = 25%, POSSIBLE = 50%, LIKELY = 75%, VERY_LIKELY = 100%
    return (index / (levels.length - 1)) * 100
  }

  /**
   * Gera detalhes descritivos da an√°lise
   */
  private generateDetails(safeSearch: any): string {
    const { adult, violence, racy } = safeSearch
    const details = []
    
    if (this.isLikelyInappropriate(adult)) {
      details.push(`Conte√∫do adulto: ${adult}`)
    }
    
    if (this.isLikelyInappropriate(violence)) {
      details.push(`Conte√∫do violento: ${violence}`)
    }
    
    if (this.isLikelyInappropriate(racy)) {
      details.push(`Conte√∫do sugestivo: ${racy}`)
    }
    
    if (details.length === 0) {
      return 'Conte√∫do apropriado'
    }
    
    return details.join(', ')
  }

  /**
   * Analisa uma URL de imagem (para casos onde temos URL em vez de buffer)
   */
  async analyzeImageUrl(imageUrl: string): Promise<ContentDetectionResult> {
    try {
      console.log('üîç CONTENT DETECTOR: Analisando URL de imagem:', imageUrl)
      
      const [result] = await this.client.safeSearchDetection({
        image: { source: { imageUri: imageUrl } }
      })

      const safeSearch = result.safeSearchAnnotation
      
      if (!safeSearch) {
        return {
          isInappropriate: false,
          confidence: 0,
          categories: {
            adult: 'VERY_UNLIKELY',
            violence: 'VERY_UNLIKELY',
            racy: 'VERY_UNLIKELY'
          },
          details: 'An√°lise n√£o dispon√≠vel'
        }
      }

      const isInappropriate = this.isContentInappropriate(safeSearch)
      const confidence = this.calculateConfidence(safeSearch)
      const details = this.generateDetails(safeSearch)

      return {
        isInappropriate,
        confidence,
        categories: {
          adult: safeSearch.adult || 'VERY_UNLIKELY',
          violence: safeSearch.violence || 'VERY_UNLIKELY',
          racy: safeSearch.racy || 'VERY_UNLIKELY'
        },
        details
      }

    } catch (error) {
      console.error('‚ùå CONTENT DETECTOR: Erro na an√°lise de URL:', error)
      
      return {
        isInappropriate: false,
        confidence: 0,
        categories: {
          adult: 'VERY_UNLIKELY',
          violence: 'VERY_UNLIKELY',
          racy: 'VERY_UNLIKELY'
        },
        details: `Erro na an√°lise: ${error instanceof Error ? error.message : 'Erro desconhecido'}`
      }
    }
  }

  /**
   * Analisa um v√≠deo (extrai frame e analisa)
   * Nota: Google Vision API n√£o analisa v√≠deos diretamente, ent√£o vamos extrair um frame
   */
  async analyzeVideo(videoUrl: string): Promise<ContentDetectionResult> {
    try {
      console.log('üîç CONTENT DETECTOR: Analisando v√≠deo:', videoUrl)
      
      // Para v√≠deos, vamos usar a URL diretamente e deixar o Google Vision API tentar
      // Na pr√°tica, ele pode analisar o thumbnail ou primeiro frame
      const [result] = await this.client.safeSearchDetection({
        image: { source: { imageUri: videoUrl } }
      })

      const safeSearch = result.safeSearchAnnotation
      
      if (!safeSearch) {
        return {
          isInappropriate: false,
          confidence: 0,
          categories: {
            adult: 'VERY_UNLIKELY',
            violence: 'VERY_UNLIKELY',
            racy: 'VERY_UNLIKELY'
          },
          details: 'An√°lise de v√≠deo n√£o dispon√≠vel'
        }
      }

      const isInappropriate = this.isContentInappropriate(safeSearch)
      const confidence = this.calculateConfidence(safeSearch)
      const details = this.generateDetails(safeSearch)

      console.log(`‚úÖ CONTENT DETECTOR: An√°lise de v√≠deo conclu√≠da - Inapropriado: ${isInappropriate}, Confian√ßa: ${confidence}%`)

      return {
        isInappropriate,
        confidence,
        categories: {
          adult: safeSearch.adult || 'VERY_UNLIKELY',
          violence: safeSearch.violence || 'VERY_UNLIKELY',
          racy: safeSearch.racy || 'VERY_UNLIKELY'
        },
        details: `An√°lise de v√≠deo: ${details}`
      }

    } catch (error) {
      console.error('‚ùå CONTENT DETECTOR: Erro na an√°lise de v√≠deo:', error)
      
      return {
        isInappropriate: false,
        confidence: 0,
        categories: {
          adult: 'VERY_UNLIKELY',
          violence: 'VERY_UNLIKELY',
          racy: 'VERY_UNLIKELY'
        },
        details: `Erro na an√°lise de v√≠deo: ${error instanceof Error ? error.message : 'Erro desconhecido'}`
      }
    }
  }
}

export default ContentDetector